services:
  atlas-worker:
    build: .
    container_name: atlas-worker
    network_mode: host
    environment:
      - ATLAS_GATEWAY_HOST=atlas
      - ATLAS_GATEWAY_PORT=18789
      - WORKER_NAME=${WORKER_NAME:-GPU Worker}
      - ATLAS_GATEWAY_TOKEN=${ATLAS_GATEWAY_TOKEN}
    volumes:
      - atlas-data:/data
      - models:/models
      - openclaw-config:/root/.openclaw
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  llama-server:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: llama-server
    network_mode: host
    volumes:
      - models:/models
    entrypoint: ["/bin/sh", "-c", "echo 'Waiting for model...' && while [ ! -f /models/current.gguf ]; do sleep 10; done && /llama-server --host 0.0.0.0 --port 8080 --model /models/current.gguf --n-gpu-layers 99 --ctx-size 8192"]
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  watchtower:
    image: containrrr/watchtower
    container_name: watchtower
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    command: --interval 300 atlas-worker llama-server
    restart: unless-stopped

volumes:
  atlas-data:
  models:
  openclaw-config:
